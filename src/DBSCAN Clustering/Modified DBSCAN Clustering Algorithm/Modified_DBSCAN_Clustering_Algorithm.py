# -*- coding: utf-8 -*-
"""Modified_DBSCAN_clustering_Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FjzefVGietvn9UWObN05BhDbK7mXm-C
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from scipy.spatial import ConvexHull
from shapely.geometry import Polygon
from google.colab import files
import time
import math
from matplotlib.lines import Line2D
from shapely.geometry import Polygon
from shapely.ops import transform, cascaded_union
import pyproj
import numpy as np

uploaded = files.upload()
data = pd.read_csv('data.csv')
np.random.seed(42)
fig, ax = plt.subplots(figsize=(8, 8))
ax.scatter(data['x'], data['y'], s=5)
ax.set_title('Distribution of Population')
ax.set_xlabel('X Distance')
ax.set_ylabel('Y Distance')
plt.show()

total_data_points = len(data)
print(f"Total data points: {total_data_points}")

def optimal_eps(data, k):
    neigh = NearestNeighbors(n_neighbors=k)
    nbrs = neigh.fit(data)
    distances, indices = nbrs.kneighbors(data)
    distances = np.sort(distances[:, k-1], axis=0)

    plt.plot(distances)
    plt.title('k-distance Graph')
    plt.xlabel('Data Points sorted by distance')
    plt.ylabel(f'{k}-th Nearest Neighbor Distance')
    plt.show()
    return distances

k = 5
distances = optimal_eps(data[['x', 'y']], k)
optimal_eps = distances[int(len(distances) * 0.9)]
print(f"Optimal eps (approx.): {optimal_eps}")

def dbscan(X, eps, min_samples, max_points_per_cluster):
    def split_clusters(X, labels, eps, min_samples, max_points_per_cluster):
        unique_labels = set(labels)
        new_labels = labels.copy()
        next_label = max(labels) + 1
        for label in unique_labels:
            if label == -1:
                continue
            points_in_cluster = X[labels == label]
            if len(points_in_cluster) > max_points_per_cluster:
                sub_db = DBSCAN(eps=eps/2, min_samples=min_samples)
                sub_labels = sub_db.fit_predict(points_in_cluster)
                sub_labels[sub_labels != -1] += next_label
                new_labels[labels == label] = sub_labels
                next_label = max(new_labels) + 1
        return new_labels

    db = DBSCAN(eps=eps, min_samples=min_samples)
    start_time = time.time()
    labels = db.fit_predict(X)
    end_time = time.time()
    while np.any(np.bincount(labels[labels != -1]) > max_points_per_cluster):
        labels = split_clusters(X, labels, eps, min_samples, max_points_per_cluster)
    print(f"Time for clustering: {end_time - start_time:.4f} seconds")
    return labels

def reclassify_noise_points(X, labels):
    noise_points = X[labels == -1]
    core_points = X[labels != -1]
    core_labels = labels[labels != -1]
    if len(noise_points) == 0:
        return labels
    neigh = NearestNeighbors(n_neighbors=1)
    neigh.fit(core_points)
    distances, indices = neigh.kneighbors(noise_points)
    new_labels = labels.copy()
    new_labels[labels == -1] = core_labels[indices.flatten()]
    return new_labels

def calculate_centroids(X, labels):
    unique_labels = set(labels)
    centroids = []
    for label in unique_labels:
        if label == -1:
            continue
        points_in_cluster = X[labels == label]
        centroid = np.mean(points_in_cluster, axis=0)
        centroids.append((label, centroid))
    return centroids

def buffered_hull(points, buffer_distance=0.05):
    if len(points) < 3:
        return Polygon(points).buffer(buffer_distance)
    hull = ConvexHull(points)
    poly_points = points[hull.vertices]
    polygon = Polygon(poly_points)
    return polygon.buffer(buffer_distance)

min_samples = 8
max_points_per_cluster = 100
X = data[['x', 'y']].values
labels = dbscan(X, optimal_eps, min_samples, max_points_per_cluster)
labels = reclassify_noise_points(X, labels)
centroids = calculate_centroids(X, labels)
unique_labels = set(labels)

colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))
plt.figure(figsize=(8, 8))
legend_elements = []

for k, col in zip(unique_labels, colors):
    class_member_mask = (labels == k)
    xy = data[class_member_mask]
    legend_elements.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=10, label=f'Cluster {k}'))

    if len(xy) > 2:
        hull_polygon = buffered_hull(xy.values)
        if hull_polygon.is_valid:
            x, y = hull_polygon.exterior.xy
            plt.plot(x, y, 'k-', linewidth=0.3)

    plt.plot(xy['x'], xy['y'], '.', markerfacecolor=tuple(col), markersize=5.5, markeredgewidth=0)

for label, centroid in centroids:
    plt.scatter(centroid[0], centroid[1], c='black', s=100, marker='x', edgecolors='w', zorder=5)

legend_elements.append(Line2D([0], [0], marker='X', color='w', markerfacecolor='black', markersize=10, label='Centroid'))
plt.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.1), fontsize='small', ncol=6)
plt.title('Modified DBSCAN Clustering Algorithm')
plt.xlabel('X distance')
plt.ylabel('Y distance')
plt.tight_layout()
plt.show()

max_lines = 9
print_data = [f"Cluster {label}: {sum(labels == label)} data points" for label in unique_labels if label != -1]

formatted_output = [""] * max_lines
for i, line in enumerate(print_data):
    column_index = i // max_lines
    row_index = i % max_lines
    formatted_output[row_index] += f"\033[1m{line:<30}\033[0m"

print("\n\033[1mDATA POINTS PER CLUSTER:\033[0m")
for line in formatted_output:
    print(line)

max_lines = 12
print_data = [f"Cluster {label}: {centroid}" for label, centroid in centroids]
formatted_output = [""] * max_lines
for i, line in enumerate(print_data):
    column_index = i // max_lines
    row_index = i % max_lines
    formatted_output[row_index] += f"\033[1m{line:<45}\033[0m"
print("\n\033[1mCENTROID OF THE CLUSTERS:\033[0m")
for line in formatted_output:
    print(line)

