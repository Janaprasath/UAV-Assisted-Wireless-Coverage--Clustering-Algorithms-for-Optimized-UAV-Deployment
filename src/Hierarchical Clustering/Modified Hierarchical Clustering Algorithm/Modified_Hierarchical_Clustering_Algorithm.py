# -*- coding: utf-8 -*-
"""Modified_Hierarchical_clustering_algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gh5CWCOwv8NKB_on4VDLsKvmnJ_xaxKD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from google.colab import files
import time
from scipy.spatial import ConvexHull
from shapely.geometry import Polygon
from matplotlib.lines import Line2D

uploaded = files.upload()
data = pd.read_csv('data.csv')
plt.figure(figsize=(8, 8))
plt.scatter(data['x'], data['y'], s=5)
plt.title('Distribution of Population')
plt.xlabel('X Distance')
plt.ylabel('Y Distance')
plt.show()

total_data_points = len(data)
print(f"Total data points: {total_data_points}")

max_cluster_size = 100
n_clusters = int(np.ceil(len(data) / max_cluster_size))
print(f'Initial number of clusters: {n_clusters}')
cluster = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
start_time_initial = time.time()
cl = cluster.fit_predict(data[['x', 'y']])
end_time_initial = time.time()
initial_clustering_time = end_time_initial - start_time_initial
print(f"Time taken for initial clustering: {initial_clustering_time:.2f} seconds")

print(f'Clusters exceeding the maximum cluster size of {max_cluster_size} data points after initial clustering:')
for label in set(cl):
    if sum(cl == label) > max_cluster_size:
        print(f"Cluster {label} has {sum(cl == label)} data points and exceeds the maximum size.")

def enforce_max_cluster_size(data, labels, max_size):
    unique_labels = set(labels)
    new_labels = labels.copy()
    next_label = max(labels) + 1

    def split_large_cluster(cluster_data, cluster_label, next_label, original_indices=None):
        if len(cluster_data) > max_size:
            sub_cluster = AgglomerativeClustering(n_clusters=2, linkage='ward')
            sub_labels = sub_cluster.fit_predict(cluster_data)
            if original_indices is None:
                original_indices = np.where(labels == cluster_label)[0]
            for sub_label in set(sub_labels):
                sub_cluster_data = cluster_data[sub_labels == sub_label]
                if len(sub_cluster_data) > max_size:
                    next_label = split_large_cluster(sub_cluster_data, next_label, next_label + 1, original_indices[sub_labels == sub_label])
                else:
                    new_labels[original_indices[sub_labels == sub_label]] = next_label
                    next_label += 1
        return next_label

    for label in unique_labels:
        if sum(labels == label) > max_size:
            cluster_data = data[labels == label]
            next_label = split_large_cluster(cluster_data, label, next_label)
    return new_labels

def calculate_centroids(data, labels):
    unique_labels = set(labels)
    centroids = []
    for label in unique_labels:
        points_in_cluster = data[labels == label]
        centroid = np.mean(points_in_cluster, axis=0)
        centroids.append((label, centroid))
    return centroids

def buffered_hull(points, buffer_distance=0.05):
    if len(points) < 3:
        return Polygon(points).buffer(buffer_distance)
    hull = ConvexHull(points)
    poly_points = points[hull.vertices]
    polygon = Polygon(poly_points)
    return polygon.buffer(buffer_distance)

start_time_enforce = time.time()
cl = enforce_max_cluster_size(data[['x', 'y']].values, cl, max_cluster_size)
end_time_enforce = time.time()
enforce_max_cluster_size_time = end_time_enforce - start_time_enforce
total_time = initial_clustering_time + enforce_max_cluster_size_time
print(f"Total time taken for clustering: {total_time:.2f} seconds")


unique_labels = sorted(set(cl))
label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
cl = np.array([label_mapping[label] for label in cl])
centroids = calculate_centroids(data[['x', 'y']].values, cl)

colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))
plt.figure(figsize=(8, 8))
legend_elements = []

for k, col in zip(unique_labels, colors):
    class_member_mask = (cl == k)
    xy = data[class_member_mask]
    legend_elements.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=10, label=f'Cluster {k}'))

    if len(xy) > 2:
        hull_polygon = buffered_hull(xy.values)
        if hull_polygon.is_valid:
            x, y = hull_polygon.exterior.xy
            plt.plot(x, y, 'k-', linewidth=0.3)

    plt.plot(xy['x'], xy['y'], '.', markerfacecolor=tuple(col), markersize=7, markeredgewidth=0)

for label, centroid in centroids:
    plt.scatter(centroid[0], centroid[1], c='black', s=100, marker='x', edgecolors='w', zorder=5)

legend_elements.append(Line2D([0], [0], marker='X', color='w', markerfacecolor='black', markersize=10, label='Centroid'))


plt.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.1), fontsize='small', ncol=6)
plt.title('Modified Agglomerative Hierarchical Clustering')
plt.xlabel('X distance')
plt.ylabel('Y distance')
plt.tight_layout()
plt.show()

unique_labels = set(cl)
centroids = calculate_centroids(data[['x', 'y']].values, cl)

max_lines = 6

print_data = [f"Cluster {label}: {sum(cl == label)} data points" for label in unique_labels if label != -1]
formatted_output = [""] * max_lines
for i, line in enumerate(print_data):
    column_index = i // max_lines
    row_index = i % max_lines
    formatted_output[row_index] += f"\033[1m{line:<30}\033[0m"
print("\n\033[1mDATA POINTS PER CLUSTER:\033[0m")
for line in formatted_output:
    print(line)

max_lines = 6
print_data = [f"Cluster {label}: {centroid}" for label, centroid in centroids]
formatted_output = [""] * max_lines
for i, line in enumerate(print_data):
    column_index = i // max_lines
    row_index = i % max_lines
    formatted_output[row_index] += f"\033[1m{line:<45}\033[0m"
print("\n\033[1mCENTROID OF THE CLUSTERS:\033[0m")
for line in formatted_output:
    print(line)

+